以用这样的一个特征向量来表示他x=(5,IT,百度,T6)。既然输入x*b*=*w*0∗*x*0其中*x*0=1变成了一个具备四个特征的向量，相对应的，仅仅一个参数w*y*=*h*(*x*)=*w*1∗*x*1+*w*2∗*x*2+*w*3∗*x*3+*w*4∗*x*4+*b*=*w*0∗*x*0+*w*1∗*x*1+*w*2∗*x*2+*w*3∗*x*3+*w*4∗*x*4就不够用了，我们应该使用4个参数每个特征对应一个。这样，我们的模型就变成



其中，*x*1对应工作年限，*x*2对应行业，对应公司，对应职级。

为了书写和计算方便，我们可以令等于b12，同时令对应于特征*w*0。由于*w*0其实并不存在，我们可以令它的值永远为1。也就是说



这样上面的式子就可以写成



我们还可以把上式写成向量的形式

*y*=*h*(*x*)=w*T*x(式1)

长成这种样子模型就叫做**线性模型**，因为输出y就是输入特征*x*1,*x*2,*x*3,...的**线性组合**。

### 监督学习和无监督学习

接下来，我们需要关心的是这个模型如何训练，也就是参数wˉ*y*(*i*)取什么值最合适。

机器学习有一类学习方法叫做**监督学习**，它是说为了训练一个模型，我们要提供这样一堆训练样本：每个训练样本既包括输入特征x(*x*(*i*),*y*(*i*))，也包括对应的输出y*E*(w)=12*n*∑*i*=1(*y*(*i*)−ˉ*y*(*i*))2=12*n*∑*i*=1(y(i)−wTx(i))2(y*E*(w)也叫做**标记，label**)。也就是说，我们要找到很多人，我们既知道他们的特征(工作年限，行业...)，也知道他们的收入。我们用这样的样本去训练模型，让模型既看到我们提出的每个问题(输入特征x*y*=*f*(*x*))，也看到对应问题的答案(标记y*f*′(*x*)=0)。当模型看到足够多的样本之后，它就能总结出其中的一些规律。然后，就可以预测那些它没看过的输入所对应的答案了。

另外一类学习方法叫做**无监督学习**，这种方法的训练样本中只有x*f*′(*x*)=0而没有y(*x*0,*y*0)。模型可以总结出特征x*x*0的一些规律，但是无法知道其对应的答案y*x*。

很多时候，既有x*x*1,*x*2,*x*3,...又有y*y*=*f*(*x*)的训练样本是很少的，大部分样本都只有xx*n**e**w*=x*o**l**d*−*η*∇*f*(*x*)。比如在语音到文本(STT)的识别任务中，x∇是语音，y∇*f*(*x*)是这段语音对应的文本。我们很容易获取大量的语音录音，然而把语音一段一段切分好并**标注**上对应文字则是非常费力气的事情。这种情况下，为了弥补带标注样本的不足，我们可以用**无监督学习方法**先做一些**聚类**，让模型总结出哪些音节是相似的，然后再用少量的带标注的训练样本，告诉模型其中一些音节对应的文字。这样模型就可以把相似的音节都对应到相应文字上，完成模型的训练。

### 线性单元的目标函数

现在，让我们只考虑**监督学习**。

在监督学习下，对于一个样本，我们知道它的特征x*f*(*x*)，以及标记y*η*。同时，我们还可以根据模型*h*(*x*)计算得到输出ˉ*y*注意这里面我们用y表示训练样本里面的**标记**，也就是**实际值**；用带上划线的ˉ*y*表示模型计算的出来的**预测值**。我们当然希望模型计算出来的ˉ*y*和y越接近越好。

数学上有很多方法来表示两者的接近程度，比如我们可以用两者的的差的平方的1/2来表示它们的接近程度

*e*=12(*y*−ˉ*y*)2

我们把ew,x叫做**单个样本**的**误差**。至于为什么前面要乘，是为了后面计算方便。

训练数据中会有很多样本，比如N∇*E*(w)个，我们可以用训练数据中**所有样本**的误差的**和**，来表示模型的误差E∇*E*(w)=∂∂w*E*(w)=∂∂w12*n*∑*i*=1(*y*(*i*)−ˉ*y*(*i*))2，也就是

*E*=*e*(1)+*e*(2)+*e*(3)+...+*e*(*n*)

上式的*e*(1)表示第一个样本的误差，*e*(2)表示第二个样本的误差......。

我们还可以把上面的式子写成和式的形式。使用和式，不光书写起来简单，逼格也跟着暴涨，一举两得。所以一定要写成下面这样

*E*=*e*(1)+*e*(2)+*e*(3)+...+*e*(*n*)=*n*∑*i*=1*e*(*i*)=12*n*∑*i*=1(*y*(*i*)−ˉ*y*(*i*))2(式2)

其中

ˉ*y*(*i*)=*h*(x(*i*))=w*T*x(i)

(式2)中，*x*(*i*)表示第i个训练样本的**特征**，*y*(*i*)表示第∂∂w(*y*(*i*)−ˉ*y*(*i*))2=2(−*y*(*i*)+ˉ*y*(*i*))x个i样本的**标记**，我们也可以用**元组**(*x*(*i*),*y*(*i*))表示第i个**训练样本**。则是模型对第i个样本的**预测值**。

我们当然希望对于一个训练数据集来说，误差最小越好，也就是(式2)的值越小越好。对于特定的训练数据集来说，的值都是已知的，所以(式2)其实是参数w的函数。



由此可见，模型的训练，实际上就是求取到合适的w，使(式2)取得最小值。这在数学上称作**优化问题**，而就是我们优化的目标，称之为**目标函数**。